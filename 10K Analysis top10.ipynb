{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of company names and their corresponding CIK codes\n",
    "pharma_companies = {\n",
    "    \"Johnson & Johnson\": \"0000200406\",\n",
    "    \"Pfizer Inc.\": \"0000078003\",\n",
    "    \"Novartis AG\": \"0001114448\",\n",
    "    \"Merck & Co., Inc.\": \"0000310158\",\n",
    "    \"Sanofi\": \"0001121404\",\n",
    "    \"GlaxoSmithKline plc\": \"0001131399\",\n",
    "    \"AbbVie Inc.\": \"0001551152\",\n",
    "    \"AstraZeneca plc\": \"0000901832\",\n",
    "    \"Bristol-Myers Squibb Company\": \"0000014272\",\n",
    "    \"Eli Lilly and Company\": \"0000059478\"\n",
    "}\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "companies_df = pd.DataFrame(list(pharma_companies.items()), columns=[\"company_name\", \"cik\"])\n",
    "\n",
    "\n",
    "\n",
    "# Define headers for SEC API access\n",
    "headers = {'User-Agent': 'Brian Lu brian901231@gmail.com'}\n",
    "\n",
    "# Add print statements to check the progress\n",
    "def get_10k_filing_links(cik):\n",
    "    \"\"\"Scrape ALL 10-K filing links from SEC EDGAR\"\"\"\n",
    "    base_url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type=10-K&count=100&output=atom\"\n",
    "    filing_links = []\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            entries = soup.find_all(\"entry\")\n",
    "            \n",
    "            for entry in entries:\n",
    "                filing_date = entry.find(\"filing-date\").text\n",
    "                filing_url = entry.find(\"filing-href\").text  # URL to filing page\n",
    "                \n",
    "                # Extract Accession Number from URL\n",
    "                parts = filing_url.split('/')\n",
    "                accession_number = parts[-1].replace('-', '')\n",
    "\n",
    "                filing_links.append({\n",
    "                    \"filing_date\": filing_date,\n",
    "                    \"accession_number\": accession_number,\n",
    "                    \"filing_url\": filing_url\n",
    "                })\n",
    "            \n",
    "            return filing_links\n",
    "        else:\n",
    "            print(f\"Failed to get historical filings for {cik}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping SEC EDGAR for {cik}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_10k_selenium(doc_url):\n",
    "    \"\"\"Extracts full 10-K text using Selenium to render JavaScript\"\"\"\n",
    "    try:\n",
    "        # Set up Chrome options\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")  # Run in headless mode (no browser UI)\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "        # âœ… Spoof User-Agent to bypass SEC bot detection\n",
    "        user_agent = \"Brian Lu brian901231@gmail.com\"\n",
    "        options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "        # Start a Chrome WebDriver session\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Open the SEC filing URL\n",
    "        print(f\"Opening {doc_url}...\")\n",
    "        driver.get(doc_url)\n",
    "\n",
    "        # Wait for the page to load completely\n",
    "        time.sleep(5)  # Adjust if needed\n",
    "\n",
    "        # Extract the main filing text\n",
    "        filing_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "        return filing_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting 10-K text from {doc_url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def extract_10k_text(filing_url):\n",
    "    \"\"\"Extract the main 10-K text from the filing HTML page\"\"\"\n",
    "    try:\n",
    "        response = requests.get(filing_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "        # Locate the \"Filing Table\" where all documents are listed\n",
    "        table = soup.find(\"table\", {\"summary\": \"Document Format Files\"})\n",
    "        if not table:\n",
    "            print(f\"Could not find filing table on page: {filing_url}\")\n",
    "            return None\n",
    "\n",
    "        # Find the row that contains \"10-K\" (the actual document)\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            cells = row.find_all(\"td\")\n",
    "            if len(cells) > 3 and \"10-K\" in cells[1].text:\n",
    "                filing_doc_link = cells[2].find(\"a\")[\"href\"]\n",
    "                break\n",
    "        else:\n",
    "            print(f\"No 10-K document found on page: {filing_url}\")\n",
    "            return None\n",
    "\n",
    "        # Construct the full document URL\n",
    "        doc_url = f\"https://www.sec.gov{filing_doc_link}\"\n",
    "        \n",
    "        # Download the 10-K document\n",
    "        doc_response = requests.get(doc_url, headers=headers)\n",
    "        if doc_response.status_code == 200:\n",
    "            if \"ix?doc\" in doc_url:\n",
    "\n",
    "                print(f\"Successfully extracted 10-K document from iXBRL format: {doc_url}\")\n",
    "                return extract_10k_selenium(doc_url)\n",
    "            else:\n",
    "                print(f\"Successfully extracted 10-K document from html format: {doc_url}\")\n",
    "                filing_text = doc_response.text\n",
    "                # Optional: Clean the text output\n",
    "                filing_text = \"\\n\".join([line.strip() for line in filing_text.split(\"\\n\") if line.strip()])\n",
    "                return filing_text\n",
    "        else:\n",
    "            print(f\"Failed to download 10-K document from {doc_url}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting 10-K text from {filing_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_10k_dataset(companies_df, start_year=2010, max_companies=None):\n",
    "    \"\"\"Create dataset with ALL historical 10-K texts for companies\"\"\"\n",
    "    \n",
    "    print(f\"\\nStarting with {len(companies_df)} companies\")\n",
    "\n",
    "    # Limit number of companies if specified\n",
    "    if max_companies:\n",
    "        companies_df = companies_df.head(max_companies)\n",
    "        print(f\"Processing first {max_companies} companies as a test\")\n",
    "\n",
    "    dataset = []\n",
    "    \n",
    "    for _, company in companies_df.iterrows():\n",
    "        cik = str(company['cik']).zfill(10)  # Ensure proper CIK format\n",
    "        print(f\"\\nProcessing {company['company_name']} (CIK: {cik})...\")\n",
    "\n",
    "        # Get ALL historical 10-K filings\n",
    "        filings = get_10k_filing_links(cik)\n",
    "\n",
    "        for filing in filings:\n",
    "            year = int(filing[\"filing_date\"][:4])\n",
    "            if year >= start_year:\n",
    "                filing_url = filing[\"filing_url\"]\n",
    "\n",
    "                # Extract 10-K text from the filing page\n",
    "                text = extract_10k_text(filing_url)\n",
    "                \n",
    "                if text:\n",
    "                    dataset.append({\n",
    "                        'company_name': company['company_name'],\n",
    "                        'cik': cik,\n",
    "                        'year': year,\n",
    "                        'filing_url': filing_url,\n",
    "                        '10k_text': text\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve 10-K text for {company['company_name']} ({year})\")\n",
    "\n",
    "        time.sleep(0.2)  # Prevent rate-limiting\n",
    "\n",
    "    # Create final DataFrame\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df = df.sort_values(['company_name', 'year'])\n",
    "    \n",
    "    print(f\"\\nDataset created with {len(df)} 10-K filings\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting with 10 companies\n",
      "Processing first 1 companies as a test\n",
      "\n",
      "Processing Johnson & Johnson (CIK: 0000200406)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2544.0_x64__qbz5n2kfra8p0\\Lib\\html\\parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 10-K document from iXBRL format: https://www.sec.gov/ix?doc=/Archives/edgar/data/200406/000020040625000038/jnj-20241229.htm\n",
      "Opening https://www.sec.gov/ix?doc=/Archives/edgar/data/200406/000020040625000038/jnj-20241229.htm...\n",
      "Failed to retrieve 10-K text for Johnson & Johnson (2025)\n",
      "Successfully extracted 10-K document from iXBRL format: https://www.sec.gov/ix?doc=/Archives/edgar/data/200406/000020040624000013/jnj-20231231.htm\n",
      "Opening https://www.sec.gov/ix?doc=/Archives/edgar/data/200406/000020040624000013/jnj-20231231.htm...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pharma_10k_df_top_10 \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_10k_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompanies_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2010\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_companies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m display(pharma_10k_df_top_10)\n",
      "Cell \u001b[1;32mIn[104], line 170\u001b[0m, in \u001b[0;36mcreate_10k_dataset\u001b[1;34m(companies_df, start_year, max_companies)\u001b[0m\n\u001b[0;32m    167\u001b[0m filing_url \u001b[38;5;241m=\u001b[39m filing[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiling_url\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Extract 10-K text from the filing page\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_10k_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiling_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text:\n\u001b[0;32m    173\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany_name\u001b[39m\u001b[38;5;124m'\u001b[39m: company[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany_name\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcik\u001b[39m\u001b[38;5;124m'\u001b[39m: cik,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10k_text\u001b[39m\u001b[38;5;124m'\u001b[39m: text\n\u001b[0;32m    179\u001b[0m     })\n",
      "Cell \u001b[1;32mIn[104], line 129\u001b[0m, in \u001b[0;36mextract_10k_text\u001b[1;34m(filing_url)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mix?doc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m doc_url:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully extracted 10-K document from iXBRL format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mextract_10k_selenium\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully extracted 10-K document from html format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[104], line 81\u001b[0m, in \u001b[0;36mextract_10k_selenium\u001b[1;34m(doc_url)\u001b[0m\n\u001b[0;32m     78\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(doc_url)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Wait for the page to load completely\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust if needed\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Extract the main filing text\u001b[39;00m\n\u001b[0;32m     84\u001b[0m filing_text \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mTAG_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "pharma_10k_df_top_10 = create_10k_dataset(companies_df, start_year=2010, max_companies=1)\n",
    "display(pharma_10k_df_top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show distinct company names and find the length\n",
    "pharma_10k_df_top_10['10k_text'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# print 10k_text with year 2025\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpharma_10k_df_top_10\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpharma_10k_df_top_10\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2025\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10k_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# print 10k_text with year 2025\n",
    "print(pharma_10k_df_top_10[pharma_10k_df_top_10['year'] == 2025]['10k_text'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
